{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 12: Learning Notebook: The Gradient Boosting Algorithm\n",
    "\n",
    "As we start to use more sophisticated algorithms, we won't spend too much time trying to explain how they work. You can investigate the details later and you learn more about ML.<P>\n",
    "\n",
    "How Boosting Works:<P>\n",
    "\n",
    "Boosting is a sequential technique which works on the principle of ensemble, which typically uses decision trees in some form. It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified (predicted) are weighted higher. This technique is followed for a classification problem while a similar technique is used for regression.\n",
    "\n",
    "Visually:\n",
    "\n",
    "<img src=\"images/gb.png\" alt=\"Gradient Boosting\" style=\"width: 400px;\"/><P>\n",
    " \n",
    "https://en.wikipedia.org/wiki/Gradient_boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and investigate the data\n",
    "We prepared this Titanic data. It should be all ready to go for classification.<P>\n",
    "\n",
    "Remember:\n",
    "- Gender: 0 = male, 1 = female\n",
    "- HasCabin: 0 = no assigned cabin on the ship (cheap ticket), 1 = has cabin (more expensive ticket)\n",
    "- C, Q, S: Dummy varibles representing the City where the passenger got on the ship\n",
    "- Survivied: 0 = died, 1 = survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>HasCabin</th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender        Age  HasCabin  C  Q  S  Survived\n",
       "0       0  22.000000         0  0  0  1         0\n",
       "1       1  38.000000         1  1  0  0         1\n",
       "2       1  26.000000         0  0  0  1         1\n",
       "3       1  35.000000         1  0  0  1         1\n",
       "4       0  35.000000         0  0  0  1         0\n",
       "5       0  29.699118         0  0  1  0         0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup boto3\n",
    "sess = boto3.session.Session()\n",
    "s3 = sess.client('s3') \n",
    "# Define the bucket & file you want to load\n",
    "source_bucket = 'machinelearning-read-only'\n",
    "source_key = 'data/titanic_clean.pkl'\n",
    "# Get the file from S3 \n",
    "response = s3.get_object(Bucket = source_bucket, Key = source_key)\n",
    "#\n",
    "# Read the 'Body' part of the response into a variable. This is where the DataFrame data exists in the response.\n",
    "body = response['Body'].read()\n",
    "#\n",
    "# Create a new pandas DataFrame using the pickle.loads() function\n",
    "titanic_df = pickle.loads(body)\n",
    "titanic_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 889 entries, 0 to 890\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Gender    889 non-null    int64  \n",
      " 1   Age       889 non-null    float64\n",
      " 2   HasCabin  889 non-null    int64  \n",
      " 3   C         889 non-null    uint8  \n",
      " 4   Q         889 non-null    uint8  \n",
      " 5   S         889 non-null    uint8  \n",
      " 6   Survived  889 non-null    int64  \n",
      "dtypes: float64(1), int64(3), uint8(3)\n",
      "memory usage: 37.3 KB\n"
     ]
    }
   ],
   "source": [
    "# Verify data types and no missing values\n",
    "titanic_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Isolate the X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = titanic_df['Survived']\n",
    "X = titanic_df.drop(['Survived'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (711, 6)\n",
      "y_train: (711,)\n",
      "X_test: (178, 6)\n",
      "y_test: (178,)\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test\n",
    "# Reserve 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "# Verify the sizes of the split datasets\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create and train a Gradient Boosting Classifier model\n",
    "Read about the hyperparameters here:<P>\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate and show performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted survivial: [0 1 1 1]\n",
      "Actual survival: [0, 1, 1, 0]\n",
      "Our model accuracy: 0.8033707865168539\n",
      "Confusion Matrix:\n",
      " [[92 21]\n",
      " [14 51]]\n"
     ]
    }
   ],
   "source": [
    "# Predict using the trained model and X_test data\n",
    "y_pred = gbc.predict(X_test)\n",
    "# Show the first 4 from predicted/actual\n",
    "print('Predicted survivial:', y_pred[0:4]) # Show a few of the predicted classes\n",
    "print('Actual survival:', list(y_test[0:4])) # Show a few of the actual classes\n",
    "# To get the accuracy, call the score() function on the trained model\n",
    "acc = gbc.score(X_test, y_test) # Number predicted correctly divided by total in data set\n",
    "print('Our model accuracy:', acc)\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Perform Hyperparameter Tuning\n",
    "Here is an example of hyperparameter tuning on the Gradient Boosting Classifier algorithm.<P>\n",
    "    \n",
    "3 interesting parameters to tune:<P>\n",
    "**n_estimators: int, default=100**\n",
    "\n",
    "    The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. Values must be in the range [1, inf).\n",
    "    \n",
    "**max_depth: int, default=3**\n",
    "\n",
    "    The maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. Values must be in the range [1, inf).\n",
    "\n",
    "**learning_rate: float, default=0.1**\n",
    "\n",
    "    Learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators. Values must be in the range (0.0, inf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to show the results from the search.\n",
    "# We'll use this below\n",
    "def display(results):\n",
    "    print(f'Best parameters are: {results.best_params_}')\n",
    "    print(\"\\n\")\n",
    "    mean_score = results.cv_results_['mean_test_score']\n",
    "    std_score = results.cv_results_['std_test_score']\n",
    "    params = results.cv_results_['params']\n",
    "    for mean,std,params in zip(mean_score,std_score,params):\n",
    "        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a default model\n",
    "gbc = GradientBoostingClassifier()\n",
    "#\n",
    "# Define the range of parameters to evaluate\n",
    "parameters = {\n",
    "    \"n_estimators\":[5,50,250,500],\n",
    "    \"max_depth\":[1,3,5,7,9],\n",
    "    \"learning_rate\":[0.01,0.1,1,10,100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing...\n",
      "177.02284145355225 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import a very powerful function called Grid Search Cross Validation.\n",
    "# It does the hyper parameter tuning for us\n",
    "#   https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#\n",
    "# Create the cv object using our model, the paramters and our k-value for k-fold cross validation\n",
    "cv = GridSearchCV(gbc,parameters,cv=5)\n",
    "#\n",
    "# Peform the grid search. This will take a while, like 3 minutes on our default instant type\n",
    "start = time.time()\n",
    "print(\"Executing...\")\n",
    "# Here it goes...\n",
    "cv.fit(X,y.values.ravel()) # Fit with the whole dataset (X,y)\n",
    "end = time.time()\n",
    "print(end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7952961340697009\n",
      "Best parameter values: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 250}\n"
     ]
    }
   ],
   "source": [
    "# Show the results of the search\n",
    "print('Best Score:', cv.best_score_)\n",
    "print('Best parameter values:', cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, show the entire results from the search\n",
    "#display(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Where to go from here\n",
    "- We tuned the parameters and found the best performing combination\n",
    "- Now, create a new gbc model using those parameters\n",
    "- Train the model using X_train and y_train\n",
    "- Evaluate the final performance\n",
    "- Use the model for predicting new passengers, if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
