{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 12 Lab: Using the XGBOOST algorithm\n",
    "\n",
    "In this lab, we are going to use a new algorithm, XGBOOST. I would consider it to be an \"industrial strength\" algoritm and not a teaching algorithm.<P>\n",
    "\n",
    "It is very similar to the Gradient Boosting algorithm we've already used but not exactly. A few details:\n",
    "- It is not included in the sklearn package\n",
    "- It is not installed on our instance in Sagemaker\n",
    "- You'll have to install it every time you restart the instance\n",
    "- It is just slightly different than other sklearn models we've used.\n",
    "\n",
    "In this lab, we will again use our abalone data. We have already prepared it for classification.\n",
    "\n",
    "The goal is the classify each abalone as 'adult' or 'youth'\n",
    "\n",
    "Recall, in the target column: adult = 1, youth = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to install XGBOOST every time we restart our instanace\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can import it\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the other stuff\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and investigate the data\n",
    "We prepared this data in an earlier module. It should be all ready to go for classification.<P>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup boto3\n",
    "sess = boto3.session.Session()\n",
    "s3 = sess.client('s3') \n",
    "# Define the bucket & file you want to load\n",
    "source_bucket = 'machinelearning-shared'\n",
    "source_key = 'data/kcolvin/abalone_clean.pkl'  # You must use your data here\n",
    "# Get the file from S3 \n",
    "response = s3.get_object(Bucket = source_bucket, Key = source_key)\n",
    "#\n",
    "# Read the 'Body' part of the response into a variable. This is where the DataFrame data exists in the response.\n",
    "body = response['Body'].read()\n",
    "#\n",
    "# Create a new pandas DataFrame using the pickle.loads() function\n",
    "abalone_df = pickle.loads(body)\n",
    "abalone_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data types and no missing values\n",
    "abalone_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Isolate the X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = abalone_df['target']\n",
    "X = abalone_df.drop(['target'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "# Reserve 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "# Verify the sizes of the split datasets\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create and train a XGB model\n",
    "\n",
    "Where to start?:\n",
    "- look for examples on the web. You might search for: xgboost classification examples\n",
    "- Glance at the XGBOOST documentation: https://xgboost.readthedocs.io/en/stable/python/python_api.html\n",
    "\n",
    "To start, just use the default hyperparameters and see if you can get it work. At the end of this step, you shold have a trained model.<P>\n",
    "\n",
    "Hint:<BR>\n",
    "xgbc = xgb.XGBClassifier()<BR>\n",
    "xgbc.fit(X_train, y_tain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate and show the model performance\n",
    "\n",
    "Just like we did with other classification algorithms, display the accuracy and the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Perform cross validation¶\n",
    "\n",
    "Recall from the previous module on cross validation the cross_val_score() function.\n",
    "\n",
    "Perform cross validation with k = 5 on your initial model and look for consistency for each fold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall from previous modules:\n",
    "# Evaluate using the whole data set: X, y\n",
    "default_xgb_scores = cross_val_score(xgbc, X, y, cv = 5)\n",
    "default_xgb_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Perform hyperparameter tuning¶\n",
    "\n",
    "3 interesting parameters to tune. They are similar to the Gradient Boosting parameters:\n",
    "- n_estimators\n",
    "- max_depth\n",
    "- learning_rate\n",
    "\n",
    "At the end of this task, you should have the best value for each of these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a default model\n",
    "xgbc = xgb.XGBClassifier()\n",
    "#\n",
    "# Define the range of parameters to evaluate\n",
    "parameters = {\n",
    "    \"n_estimators\":[5,50,250,500],\n",
    "    \"max_depth\":[1,3,5,7,9],\n",
    "    \"learning_rate\":[0.01,0.1,1,10,100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to show the results from the search.\n",
    "def display(results):\n",
    "    print(f'Best parameters are: {results.best_params_}')\n",
    "    print(\"\\n\")\n",
    "    mean_score = results.cv_results_['mean_test_score']\n",
    "    std_score = results.cv_results_['std_test_score']\n",
    "    params = results.cv_results_['params']\n",
    "    for mean,std,params in zip(mean_score,std_score,params):\n",
    "        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, show the entire results from the search\n",
    "display(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Using the best parameter values above, train a new model and make predictions¶\n",
    "\n",
    "    Predict if the following abalone is 'adult' or 'youth'\n",
    "\n",
    "a = [1.0, 0.435, 0.395, 0.090, 0.534, 0.1245, 0.131, 0.25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
