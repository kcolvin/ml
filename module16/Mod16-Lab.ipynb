{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 16: Lab: Even more PCA\n",
    "\n",
    "You try it with an even wider dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "import boto3\n",
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the gene expression, very wide dataset\n",
    "sess = boto3.session.Session()\n",
    "s3 = sess.client('s3') \n",
    "source_bucket = 'machinelearning-read-only' \n",
    "source_key = 'data/gene_expression_wide.pkl'\n",
    "#\n",
    "# Get the file from S3 \n",
    "response = s3.get_object(Bucket = source_bucket, Key = source_key)\n",
    "#\n",
    "# Read the 'Body' part of the response into a variable. This is where the DataFrame data exists in the response.\n",
    "body = response['Body'].read()\n",
    "#\n",
    "# Create a new pandas DataFrame using the pickle.loads() function\n",
    "df = pickle.loads(body)\n",
    "print('Dataset size:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now only focused on the feature matrix:\n",
    "X = df.drop(['target'], axis = 1)\n",
    "print('Dataset size:', X.shape)\n",
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how much variance is captured in each PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You decide how many features you can plot in the graph below. \n",
    "# There is a maximum that is much lower than 20,532 columns\n",
    "max_features = 'your integer value here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a scaler-PCA pipeline\n",
    "#\n",
    "scaler = StandardScaler() # Standardize the data\n",
    "# Create a PCA model\n",
    "pca = PCA(n_components = max_features,random_state = 11) \n",
    "#\n",
    "steps = [('Scaler', scaler), ('PCA', pca)]\n",
    "# Create a pipeline\n",
    "pipe = Pipeline(steps)\n",
    "# Fit\n",
    "pipe.fit(X)\n",
    "# Transform\n",
    "principle_array = pipe.transform(X)\n",
    "# Build a df\n",
    "principal_df = pd.DataFrame(data = principle_array)\n",
    "#\n",
    "# Show the captured variance curve\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many Principal Components will you include?\n",
    "- Set the n_components to the minimum percent of variance you want to capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You decide how much variance to capture:\n",
    "min_var = 'Your decimal value here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a scaler-PCA pipeline\n",
    "#\n",
    "scaler = StandardScaler() # Standardize the data\n",
    "# Create a PCA model\n",
    "# Specify the percent of variance you want to capture\n",
    "pca = PCA(n_components = min_var, random_state = 11) # make sure we discuss n_componets = .9\n",
    "#\n",
    "steps = [('Scaler', scaler), ('PCA', pca)]\n",
    "# Create a pipeline\n",
    "pipe = Pipeline(steps)\n",
    "# Fit\n",
    "pipe.fit(X)\n",
    "# Transform\n",
    "principle_array = pipe.transform(X)\n",
    "# Build a df\n",
    "pca_df = pd.DataFrame(data = principle_array)\n",
    "# Reduced\n",
    "print('Shape of new feature matrix:', pca_df.shape)\n",
    "print('We captured this percent of variance:', round(sum(pca.explained_variance_ratio_),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform machine learning process on reduced dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = pca_df # The reduced dataset\n",
    "# Target: Did the patient have cancer?\n",
    "y = df['target'] # Create a series\n",
    "# Split into train/test\n",
    "# Reserve 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,random_state = 42)\n",
    "# Verify the sizes of the split datasets\n",
    "print('X_train:', X_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('X_test:', X_test.shape)\n",
    "print('y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a pipeline perform modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to predict which type of cancer was detected\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SVM Muticlass Classification\n",
    "scaler = StandardScaler()\n",
    "svm_mc = svm.SVC()\n",
    "steps = [('Scaler', scaler), ('SVM', svm_mc)]\n",
    "pipe = Pipeline(steps)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "acc = svm_mc.score(X_test, y_test) # Number predicted correctly divided by total in data set\n",
    "print('Our model accuracy:', round(acc,4))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = ['PRAD', 'LUAD', 'BRCA', 'KIRC', 'COAD'], \n",
    "                     columns = ['PRAD', 'LUAD', 'BRCA', 'KIRC', 'COAD'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's discuss this output\n",
    "# We can refer back to Notebook 14-1-SVM for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
