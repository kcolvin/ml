{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module 15 - 1:  Using the XGBOOST on AWS SageMaker\n",
    "\n",
    "In this module, we will use the XGBOOST algorithm using SageMaker's implementation.<P>\n",
    "\n",
    "This will be very different than our past work using sklearn libraries and local processing on our SageMaker Studio IDE.<P>\n",
    "    \n",
    "This takes both machine learning and AWS knowledge.<P>\n",
    "    \n",
    "Quickly review pricing:\n",
    "- https://aws.amazon.com/sagemaker/pricing/\n",
    "       \n",
    "**Data**: We will use a familiar dataset: Abalone\n",
    "\n",
    "Recall:\n",
    "- The predictors, X, are physical measurements of the abalone\n",
    "- In the target column: adult = 1, youth = 0. We are trying to predict if the abalone is adult or youth.<P>\n",
    " \n",
    "Very long process we will follow:<P>\n",
    "1. Load and investigate the data\n",
    "2. Prepare data for XGBOOST on SM\n",
    "3. Splitting into 3 sets: Train, Test and Evaluate\n",
    "4. Upload prepared data to S3\n",
    "5. Setup Sagemaker training channels\n",
    "6. Create Sagemaker XGBoost Model\n",
    "7. Train the model (**Costs money**)\n",
    "8. Start the model: Also called a inference instance, predictor or endpoint (**Costs money**)\n",
    "9. Use the endpoint to evaluate performance\n",
    "10. Predict on unseen data\n",
    "11. Delete your endpoint so we don't have to pay for it anymore...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from io import StringIO\n",
    "import sagemaker\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and investigate the data\n",
    "We prepared this data in an earlier module. It should be all ready to go for classification.<P>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole weight</th>\n",
       "      <th>shucked weight</th>\n",
       "      <th>viscera weight</th>\n",
       "      <th>shell weight</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  length  diameter  height  whole weight  shucked weight  \\\n",
       "0    0   0.455     0.365   0.095        0.5140          0.2245   \n",
       "1    0   0.350     0.265   0.090        0.2255          0.0995   \n",
       "2    1   0.530     0.420   0.135        0.6770          0.2565   \n",
       "\n",
       "   viscera weight  shell weight  target  \n",
       "0          0.1010          0.15       1  \n",
       "1          0.0485          0.07       0  \n",
       "2          0.1415          0.21       0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup boto3\n",
    "sess = boto3.session.Session()\n",
    "s3 = sess.client('s3') \n",
    "source_bucket = 'machinelearning-read-only'\n",
    "source_key = 'data/abalone_clean.pkl' \n",
    "response = s3.get_object(Bucket = source_bucket, Key = source_key)\n",
    "#\n",
    "body = response['Body'].read()\n",
    "#\n",
    "# Create a new pandas DataFrame using the pickle.loads() function\n",
    "abalone_df = pickle.loads(body)\n",
    "abalone_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2835 entries, 0 to 2834\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   sex             2835 non-null   int64  \n",
      " 1   length          2835 non-null   float64\n",
      " 2   diameter        2835 non-null   float64\n",
      " 3   height          2835 non-null   float64\n",
      " 4   whole weight    2835 non-null   float64\n",
      " 5   shucked weight  2835 non-null   float64\n",
      " 6   viscera weight  2835 non-null   float64\n",
      " 7   shell weight    2835 non-null   float64\n",
      " 8   target          2835 non-null   int64  \n",
      "dtypes: float64(7), int64(2)\n",
      "memory usage: 199.5 KB\n"
     ]
    }
   ],
   "source": [
    "# Verify data types are numeric and no missing values\n",
    "abalone_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare data for XGBOOST on SM\n",
    "\n",
    "XGBoost on SageMaker requires the data to be in a single file. The file must have the target value be the first column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>sex</th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole weight</th>\n",
       "      <th>shucked weight</th>\n",
       "      <th>viscera weight</th>\n",
       "      <th>shell weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.7775</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  sex  length  diameter  height  whole weight  shucked weight  \\\n",
       "0       1    0   0.455     0.365   0.095        0.5140          0.2245   \n",
       "1       0    0   0.350     0.265   0.090        0.2255          0.0995   \n",
       "2       0    1   0.530     0.420   0.135        0.6770          0.2565   \n",
       "3       1    0   0.440     0.365   0.125        0.5160          0.2155   \n",
       "4       1    1   0.530     0.415   0.150        0.7775          0.2370   \n",
       "\n",
       "   viscera weight  shell weight  \n",
       "0          0.1010         0.150  \n",
       "1          0.0485         0.070  \n",
       "2          0.1415         0.210  \n",
       "3          0.1140         0.155  \n",
       "4          0.1415         0.330  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reorder the columns and put the  target into the first position.\n",
    "df = abalone_df[['target','sex','length','diameter','height',\n",
    "                 'whole weight','shucked weight','viscera weight','shell weight']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Splitting into 3 sets: Train, Test and Evaluate\n",
    "\n",
    "We will start by splitting the dataset into two datasets. Then, we will split the 2nd dataset into 2 datasets.\n",
    "\n",
    "- train: 80% of the data used to train the model\n",
    "- validate: 10% of the data used to validate the training model\n",
    "- test: 10% of the data held out for us to evaluate the model performance on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (2268, 9)\n",
      "Validatate: (284, 9)\n",
      "Test: (283, 9)\n"
     ]
    }
   ],
   "source": [
    "# Use train_test_split twice\n",
    "train, test_and_validate = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "test, validate = train_test_split(test_and_validate, test_size=0.5, \n",
    "                                  random_state=42, stratify=test_and_validate['target'])\n",
    "print('Training:', train.shape) # Training data\n",
    "print('Validatate:', validate.shape) # Validate during training\n",
    "print('Test:', test.shape) # Saved for us to test with at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1467\n",
      "0     801\n",
      "Name: target, dtype: int64\n",
      "1    184\n",
      "0    100\n",
      "Name: target, dtype: int64\n",
      "1    183\n",
      "0    100\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Quick check on evenly distributed target vavlues\n",
    "print(train['target'].value_counts())\n",
    "print(validate['target'].value_counts())\n",
    "print(test['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Upload prepared data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will help us upload the data to S3\n",
    "def upload(key, bucket, dataframe):\n",
    "    csv_buffer = StringIO()\n",
    "    dataframe.to_csv(csv_buffer, header=False, index=False)\n",
    "    csv_buffer.seek(0)\n",
    "    response = s3.put_object(Bucket = bucket, Body = csv_buffer.getvalue(), Key = key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to S3\n",
    "# Define some variables to store the data\n",
    "#\n",
    "# This is a bucket in which you have full access\n",
    "destination_bucket = 'machinelearning-shared'\n",
    "#\n",
    "# Keys: Modify for your use\n",
    "train_file='data/kcolvin/sagemaker/abalone_train.csv'\n",
    "validate_file='data/kcolvin/sagemaker/abalone_validate.csv'\n",
    "test_file='data/kcolvin/sagemaker/abalone_test.csv'\n",
    "#\n",
    "# Use the function from above to upload to S3\n",
    "upload(train_file,destination_bucket,train)\n",
    "upload(validate_file,destination_bucket,validate)\n",
    "upload(test_file,destination_bucket,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/kcolvin/sagemaker/abalone_test.csv\n",
      "data/kcolvin/sagemaker/abalone_train.csv\n",
      "data/kcolvin/sagemaker/abalone_validate.csv\n"
     ]
    }
   ],
   "source": [
    "# Verify the files are in the correct location\n",
    "response = s3.list_objects(Bucket = destination_bucket)\n",
    "# Parse though the response\n",
    "# Modify for your use\n",
    "my_folder = 'kcolvin/sagemaker/abalone' # A filter to look for only files in my folder\n",
    "for object in response['Contents']:\n",
    "    if my_folder in object['Key']: # This is looking for a substring in the object['Key'] string\n",
    "        print(object['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Setup Sagemaker training channels\n",
    "This this is how the SM model obtains the data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machinelearning-shared\n",
      "data/kcolvin/sagemaker/abalone_train.csv\n",
      "data/kcolvin/sagemaker/abalone_validate.csv\n",
      "s3://machinelearning-shared/data/kcolvin/sagemaker/abalone_train.csv\n",
      "s3://machinelearning-shared/data/kcolvin/sagemaker/abalone_validate.csv\n",
      "{'train': <sagemaker.inputs.TrainingInput object at 0x7f0b367a2fd0>,\n",
      " 'validation': <sagemaker.inputs.TrainingInput object at 0x7f0b367a2e50>}\n"
     ]
    }
   ],
   "source": [
    "# Setup channels\n",
    "# Recall, we've setup these variables\n",
    "print(destination_bucket)\n",
    "print(train_file)\n",
    "print(validate_file)\n",
    "\n",
    "# Define train and validate channels\n",
    "train_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}\".format(destination_bucket,train_file),\n",
    "    content_type='text/csv')\n",
    "print(\"s3://{}/{}\".format(destination_bucket,train_file))\n",
    "\n",
    "validate_channel = sagemaker.inputs.TrainingInput(\n",
    "    \"s3://{}/{}\".format(destination_bucket,validate_file),\n",
    "    content_type='text/csv')\n",
    "print(\"s3://{}/{}\".format(destination_bucket,validate_file))\n",
    "#\n",
    "# Finally, setup a dictionary pointing to the channels (required for SM xgboost)\n",
    "data_channels = {'train': train_channel, 'validation': validate_channel}\n",
    "pprint.pprint(data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create Sagemaker XGBoost Model\n",
    "Documentation:<BR>\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/ecr-us-west-2.html#xgboost-us-west-2.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-west-2'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will work in the 'us-west-2' region\n",
    "reg = sess.region_name\n",
    "reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-xgboost:1.5-1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will get us a pointer to a 'container'\n",
    "# Discuss containers:\n",
    "#  https://www.docker.com/resources/what-container/\n",
    "#  https://sagemaker-workshop.com/custom/containers.html\n",
    "from sagemaker.image_uris import retrieve\n",
    "#\n",
    "#container = retrieve('xgboost',boto3.Session().region_name,'1.0-1') # Old version of model\n",
    "container = retrieve('xgboost',reg,'1.5-1')\n",
    "print(type(container))\n",
    "container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://machinelearning-shared/data/kcolvin/sagemaker/output/\n"
     ]
    }
   ],
   "source": [
    "# Set a location to ouput model artifacts (files)\n",
    "my_prefix = 'data/kcolvin/sagemaker'\n",
    "\n",
    "s3_output_location=\"s3://{}/{}/output/\".format(destination_bucket,my_prefix)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.estimator.Estimator at 0x7f0b35df1cd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Sagemaker Estimator (this is creating the 'model')\n",
    "# Setup job_name\n",
    "my_job = 'kcolvin'\n",
    "#\n",
    "xgb_model=sagemaker.estimator.Estimator(container, # Reference to the container, defined above \n",
    "                                        sagemaker.get_execution_role(),\n",
    "                                        instance_count=1,\n",
    "                                        instance_type= 'ml.m5.large', # smallest avaialbe\n",
    "                                        output_path=s3_output_location,\n",
    "                                        sagemaker_session=sagemaker.session.Session(),\n",
    "                                        base_job_name = my_job) \n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Currently, all hyperparameters are set to the default value\n",
    "#   https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html\n",
    "#\n",
    "xgb_model.hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_round': 42, 'eval_metric': 'error', 'objective': 'binary:logistic'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can set hyperparameters like this:\n",
    "#\n",
    "xgb_model.set_hyperparameters(\n",
    "    num_round = 42,\n",
    "    eval_metric = \"error\",\n",
    "    objective = \"binary:logistic\")\n",
    "# Show the non-default parameters\n",
    "xgb_model.hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Train the model\n",
    "OK, here comes the recognized function: model.fit().<P>\n",
    "    \n",
    "This will launch the container and train the model using the train & validate data. <P>\n",
    "    \n",
    "It takes 4 or 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing...\n",
      "\n",
      "2022-09-02 18:19:20 Starting - Starting the training job...\n",
      "2022-09-02 18:19:35 Starting - Preparing the instances for training.............\n",
      "2022-09-02 18:20:49 Downloading - Downloading input data...........\n",
      "2022-09-02 18:21:49 Training - Downloading the training image........\n",
      "2022-09-02 18:22:35 Training - Training image download completed. Training in progress...\n",
      "2022-09-02 18:22:50 Uploading - Uploading generated training model...\n",
      "2022-09-02 18:23:07 Completed - Training job completed\n",
      "231.5843222141266 seconds\n"
     ]
    }
   ],
   "source": [
    "# Only run this cell once. \n",
    "#  It will start a new training container everytime you run it.\n",
    "start = time.time()\n",
    "print(\"Executing...\")\n",
    "xgb_model.fit(inputs=data_channels, logs=False)\n",
    "end = time.time()\n",
    "print(end - start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kcolvin-2022-09-02-18-19-20-018\n",
      "gmcgregor-2022-09-02-18-14-38-411\n",
      "georgelund-2022-09-02-18-14-11-475\n",
      "kcolvin-2022-09-02-18-13-48-345\n",
      "joshtolle-2022-09-02-18-13-43-993\n",
      "mmacaraeg-2022-09-02-18-13-21-998\n",
      "lou-2022-09-02-18-09-25-441\n",
      "kcolvin-2022-09-02-17-35-53-093\n",
      "student1-2022-08-31-17-01-27-906\n",
      "kcolvin--2022-08-31-15-10-53-664\n"
     ]
    }
   ],
   "source": [
    "# After complete, we can see the past training jobs. We paid a few minutes for each of these jobs.\n",
    "# Create a Sagemaker client\n",
    "sm = sess.client('sagemaker')\n",
    "for job in sm.list_training_jobs()['TrainingJobSummaries']:\n",
    "    print(job['TrainingJobName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/george.lund/sagemaker/output/georgelund-2022-09-02-18-14-11-475/output/model.tar.gz\n",
      "data/gmcgregor/sagemaker/output/gmcgregor-2022-09-02-18-14-38-411/output/model.tar.gz\n",
      "data/joshtolle/sagemaker/output/joshtolle-2022-09-02-18-13-43-993/output/model.tar.gz\n",
      "data/kcolvin/sagemaker/output/kcolvin--2022-08-30-20-15-31-507/output/model.tar.gz\n",
      "data/kcolvin/sagemaker/output/kcolvin--2022-08-30-21-11-52-206/output/model.tar.gz\n",
      "data/kcolvin/sagemaker/output/kcolvin--2022-08-30-22-11-25-279/output/model.tar.gz\n",
      "data/kcolvin/sagemaker/output/kcolvin--2022-08-31-15-10-53-664/output/model.tar.gz\n",
      "data/kcolvin/sagemaker/output/kcolvin-2022-09-02-17-35-53-093/output/model.tar.gz\n",
      "data/kcolvin/sagemaker/output/kcolvin-2022-09-02-18-13-48-345/output/model.tar.gz\n",
      "data/kcolvin/sagemaker/output/kcolvin-2022-09-02-18-19-20-018/output/model.tar.gz\n",
      "data/kcolvin/sagemaker/output/sagemaker-xgboost-2022-08-30-17-47-33-158/output/model.tar.gz\n",
      "data/lou/sagemaker/output/lou-2022-09-02-18-09-25-441/output/model.tar.gz\n",
      "data/mmacaraeg/sagemaker/output/mmacaraeg-2022-09-02-18-13-21-998/output/model.tar.gz\n",
      "data/student1/sagemaker/output/student1-2022-08-31-17-01-27-906/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# We can also see the file the trained model was stored\n",
    "response = s3.list_objects(Bucket = destination_bucket)\n",
    "# Modify for your use\n",
    "my_folder = 'model.tar.gz' # A filter\n",
    "for object in response['Contents']:\n",
    "    if my_folder in object['Key']: # This is looking for a substring in the object['Key'] string\n",
    "        print(object['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Start the model: Also called a inference instance, predictor or endpoint\n",
    "This takes about 5 or 6 minutes to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing...\n",
      "--------!\n",
      " 241.865980386734 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.predictor.Predictor at 0x7f0b37c18e50>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start inference instance (or predictor)\n",
    "#\n",
    "# Name your endpoint. This must be unique\n",
    "my_endpoint = 'kcolvin-endpoint10'\n",
    "#\n",
    "start = time.time()\n",
    "print(\"Executing...\")\n",
    "xgb_predictor = xgb_model.deploy(initial_instance_count=1,\n",
    "                                 serializer = sagemaker.serializers.CSVSerializer(), # How to input data\n",
    "                                 #deserializer= sagemaker.deserializers.JSONDeserializer(), # return JSON\n",
    "                                 deserializer= sagemaker.deserializers.CSVDeserializer(), # return CSV\n",
    "                                 instance_type='ml.t2.medium', # smallest possible instance\n",
    "                                 endpoint_name = my_endpoint # must be a unique name\n",
    "                                )\n",
    "end = time.time()\n",
    "print('\\n',end - start, 'seconds')\n",
    "xgb_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Use the endpoint to evaluate performance\n",
    "Warning: The output of the model is a little confusing. Some coding required...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0.972118616104126']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the validate data to see how the model perfomred during training\n",
    "#\n",
    "# To predict, we need to drop 'target' from the validate data\n",
    "X_test = validate.drop('target', axis = 1) \n",
    "X_test_array = X_test.to_numpy() # Need convert the dataframe to a numpy array\n",
    "# Just predict one row of data\n",
    "p = xgb_predictor.predict(X_test_array[0])\n",
    "# Check out the datatype: list of lists of strings!\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first abalone in the validate set is adult: 1\n"
     ]
    }
   ],
   "source": [
    "# Sagemaker XGBOOST returns a the probability the value is True. \n",
    "# Let's just convert this to a 0 or 1\n",
    "# First, strip the lists, then convert to a floating point\n",
    "p_prob = float(p[0][0])\n",
    "# Now convert it to a 1 or 0\n",
    "if p_prob < .5:\n",
    "    adult = 0\n",
    "else:\n",
    "    adult = 1\n",
    "print('The first abalone in the validate set is adult:', adult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284, 8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict for the whole array\n",
    "y_pred = xgb_predictor.predict(X_test_array)\n",
    "# Convert the list of list of strings to a list of integers (0,1)\n",
    "y_lst = []\n",
    "for i in y_pred:\n",
    "    if float(i[0]) < .5:  # Could modify this, if wanted\n",
    "        y_lst.append(0)\n",
    "    else:\n",
    "        y_lst.append(1)\n",
    "# Now we have a list of predicted 1 (adults) or 0 (youth)\n",
    "y_pred = pd.Series(y_lst) # convert it back to a pandas series\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.7323943661971831\n",
      "Confusion Matrix:\n",
      " [[ 57  43]\n",
      " [ 33 151]]\n"
     ]
    }
   ],
   "source": [
    "# Performance from training data\n",
    "# These metrics should look familiar\n",
    "y_test = validate['target']\n",
    "print('Model accuracy score:', accuracy_score(y_test,y_pred))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "# What now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Predict on unseen data\n",
    "Remember, we held out 10% of the data to see how the model does on non-training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.7597173144876325\n",
      "Confusion Matrix:\n",
      " [[ 57  43]\n",
      " [ 25 158]]\n"
     ]
    }
   ],
   "source": [
    "# Now, do the same thing using the test dataset\n",
    "X_test = test.drop('target', axis = 1) \n",
    "X_test_array = X_test.to_numpy() # Need convert the dataframe to a numpy array\n",
    "y_pred = xgb_predictor.predict(X_test_array)\n",
    "y_lst = []\n",
    "for i in y_pred:\n",
    "    if float(i[0]) < .5:  # Could modify this, if wanted\n",
    "        y_lst.append(0)\n",
    "    else:\n",
    "        y_lst.append(1)\n",
    "y_pred = pd.Series(y_lst) # convert it back to a pandas series\n",
    "# Performance from test data\n",
    "y_test = test['target']\n",
    "print('Model accuracy score:', accuracy_score(y_test,y_pred))\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Delete your endpoint so we don't have to pay for it anymore..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '44c0415f-b4a5-46c4-819e-421d9f146966',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '44c0415f-b4a5-46c4-819e-421d9f146966',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Wed, 31 Aug 2022 16:31:16 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When you are ready, uncomment the next line and delete your endpoint\n",
    "#sm.delete_endpoint(EndpointName = my_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
